{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUS 41204 Machine Learning\n",
    "## Winter 2023 Midterm\n",
    "\n",
    "### Yazmin Ramirez Delgado\n",
    "#### yazramirez@uchicago.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Question [30 points]\n",
    "\n",
    "The file eBayAuctions.csv contains information on 1972 auctions transacted on eBay.com during Mayâ€“June 2004. The goal is to use these data to build a model that will distinguish competitive auctions from noncompetitive ones. A competitive auction is defined as an auction with at least two bids placed on the item being auctioned. The data include variables that describe the item (auction category), the seller (his or her eBay rating), and the auction terms that the seller selected (auction duration, opening price, currency, day of week of auction close). In addition, we have the price at which the auction closed. The goal is to predict whether or not an auction of interest will be competitive.\n",
    "\n",
    "Partition the data into training (60%) and validation (40%) set.\n",
    "\n",
    "1. Discuss if you can use all the variables to predict at the start of an auction whether it will be competitive.\n",
    "2. Build a classification tree, a boosted tree, a bagged tree, and a random forest model (with mtry = 4). Choose the tuning parameters for these models by optimizing the performance on the validation set. Report accuracies of these four models as well as their confusion matrices on the validation set.\n",
    "3. Create lift curves for these four models. What is the lift for the top 10% of observations in the validation set?\n",
    "4. Briefly describe how boosted tree, bagged tree and random forest models are conceptually similar and how are they conceptually different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The confusion matrix is a table that presents the performance of a binary classification model. It shows the number of correct and incorrect predictions made by the model, by comparing its predictions with the actual outcomes in a dataset. The confusion matrix consists of four categories: true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "b) In this case, the confusion matrix for the consultant's model would have two categories: \"fraud predicted\" and \"fraud not predicted\". The consultant's model would predict either \"fraud\" or \"no fraud\" for each account. The confusion matrix would then be filled out based on the actual outcome for each account. If the account was fraudulent and the model predicted fraud, it would be a true positive. If the account was not fraudulent and the model predicted fraud, it would be a false positive. If the account was not fraudulent and the model predicted no fraud, it would be a true negative. If the account was fraudulent and the model predicted no fraud, it would be a false negative.\n",
    "\n",
    "c) The cost/benefit matrix is a table that presents the costs and benefits associated with the different outcomes in a classification problem. In this case, the cost/benefit matrix would show the financial costs and benefits of correctly and incorrectly classifying an account as fraudulent or not fraudulent. The cost/benefit matrix would help to determine the optimal threshold for the model to use in making its predictions.\n",
    "\n",
    "d) The confusion matrix and cost/benefit matrix are important for this problem because they provide a clear understanding of the performance of the model and the financial implications of its decisions. By analyzing the confusion matrix and cost/benefit matrix, we can determine if the model is performing well and if its decisions align with the business objectives.\n",
    "\n",
    "e) The proper evaluation function for the consultant's model is the F1 score, which is a weighted average of precision and recall. The F1 score is calculated as follows: F1 = 2 * (precision * recall) / (precision + recall), where precision is the number of true positives divided by the sum of true positives and false positives, and recall is the number of true positives divided by the sum of true positives and false negatives.\n",
    "\n",
    "f) The confusion and cost matrices come into play in the F1 score calculation because they provide the necessary information to calculate precision and recall. Precision is the ratio of true positives to the sum of true positives and false positives, which are both present in the confusion matrix. Recall is the ratio of true positives to the sum of true positives and false negatives, which are also present in the confusion matrix. The cost/benefit matrix provides the financial implications of each of these outcomes, which can be used to determine the optimal threshold for the model to use in making its predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
