{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Homework 4\n",
    "## BUSN 41204 - 2023\n",
    "\n",
    "- Aman Krishna\n",
    "- Christian Pavilanis\n",
    "- Jingwen Li\n",
    "- Yazmin Ramirez Delgado  \n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "For each statement write if it is true or false and provide a one sentence explanation. You will get points only if your explanation is correct.\n",
    "\n",
    "#### 1. \n",
    "Subset selection hierarchy:\n",
    "\n",
    "a. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection.\n",
    "\n",
    "R.\n",
    "\n",
    "> True. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the models, one at a time until all of the predictors are in the model.\n",
    "\n",
    "b. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by backward stepwise selection.\n",
    "\n",
    "R.\n",
    "\n",
    "> True. The model with k predictors is obtained by removing one predictor from the model with (k+1) predictors.\n",
    "\n",
    "\n",
    "c. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.\n",
    "\n",
    "R.\n",
    "\n",
    "> False. The model with (k+1) predictors is obtained by selecting among all possible models with (k+1) predictors, and so does not necessarily contain all the predictors selected for the k-variable model.\n",
    "\n",
    "d. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection.\n",
    "\n",
    "R. \n",
    "\n",
    "> False. It is possible that the best k-variable model selected by best subset selection is a subset of the (k + 1)-variable model selected by forward stepwise selection, but it is not guaranteed. \n",
    "\n",
    "e. The 1-variable model identified by best subset is the same as identified by forward stepwise selection.\n",
    "\n",
    "R.\n",
    "\n",
    "> True. Both best subset selection and forward stepwise selection start by considering the addition of a single variable to the model. In this case, the 1-variable model would consist of the variable that results in the best fit among all the variables being considered.\n",
    "\n",
    "f. The 1-variable model identified by backward stepwise is the same as identified by forward stepwise selection.\n",
    "\n",
    "R. \n",
    "\n",
    ">False. There is no direct link between the models obtained from forward and backward selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \n",
    "The following plot shows the cross validation scores for a ridge regression model with different values of the tuning parameter λ. Note that here what is plotted as the x-axis is the ratio of the l2 norms of the ridge regression estimate $ \\widehat \\beta^{R}_{\\lambda}$ and the least squares estimate $ \\widehat \\beta $.  The two dashed lines correspond to two values of λ, and hence two models.\n",
    "\n",
    "a. Model I uses a larger λ than Model II.\n",
    "\n",
    "R. \n",
    "\n",
    ">True, the ratio of the l2 norms of the ridge regression estimate $ \\widehat \\beta^{R}_{\\lambda}$ and the least squares estimate $ \\widehat \\beta $ gives us an idea of how much the coefficients of the ridge regression model deviate from the coefficients of the least squares model as a result of the regularization applied through the tuning parameter λ. A ratio close to 1 would indicate that the coefficients of the ridge regression model are similar to the coefficients of the least squares model, and hence, not much regularization is applied. On the other hand, a ratio much less than 1 would indicate that the coefficients of the ridge regression model are significantly different from the coefficients of the least squares model, and hence, a substantial amount of regularization has been applied. Thus Model I uses a larger λ than Model II.\n",
    "\n",
    "b. Model I has a higher bias than Model II.\n",
    "\n",
    "R.\n",
    "\n",
    "> True, If two ridge regression models have the same cross-validation scores, but one has a larger value of λ than the other, the model with the larger value of λ would generally have a higher bias. A larger value of λ corresponds to stronger regularization and a simpler model, which tends to have higher bias. This is because stronger regularization reduces the magnitude of the coefficients, which can result in a model that is less flexible and may not capture the underlying structure of the data as well as a model with weaker regularization. On the other hand, a model with a smaller value of λ and weaker regularization would have lower bias, but may be more prone to overfitting the data and have higher variance. \n",
    "\n",
    "c. Model I has a larger in-sample RSS than Model II.\n",
    "\n",
    "R.\n",
    "\n",
    "> False, it is difficult to determine which of two models with the same cross-validation scores would have a larger or smaller in-sample RSS based solely on the value of λ. This is because the in-sample RSS is dependent on both the model structure and the data, and can be affected by many other factors such as the number of features, the choice of features, the presence of outliers, etc. A model with weaker regularization and a smaller value of λ may have a smaller in-sample RSS due to its increased flexibility and ability to capture more of the underlying structure of the data. However, as mentioned earlier, this also increases the risk of overfitting the data and can result in a model with higher variance.\n",
    "\n",
    "d. Model I may have a larger test error than Model II.\n",
    "\n",
    "R.\n",
    "\n",
    "> True, Model I has a larger λ which corresponds to stronger regularization, this reduces the magnitude of the coefficients and results in a simpler and less flexible model. A simpler model may not fit the test data as well as a model with weaker regularization, leading to a higher test error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "You can download files for this assignment from http://www.kaggle.com/competitions/busn41204-winter23-homework-4\n",
    "\n",
    "The data set contains prices of residential homes in Ames, Iowa. The data were cleaned and the categorical variables were converted to indicators.\n",
    "\n",
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from boruta import BorutaPy\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
